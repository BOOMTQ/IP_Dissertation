\documentclass[ oneside,% the name of the author
                    author={Cassie Qing Tang},
                % the degree programme: BSc, MEng, MSci or MSc.
                    degree={BSc},
                % the dissertation    title (which cannot be blank)
                     title={An Automated Response System for Disrupting Online Pet Scamming \\ },
                % the dissertation subtitle (which can    be blank)
                    subtitle={ }]{dissertation}

\usepackage[english]{babel}
\usepackage[nottoc]{tocbibind}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{float}
\usepackage[list=true]{subcaption}
\usepackage{caption}
\captionsetup[figure]{labelformat=empty} % This disables automatic labelling of graphics
\captionsetup[subfigure]{labelformat=empty} % This disables automatic labelling of graphics
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{fancyvrb}


\begin{document}

\maketitle
% \fronmatter

\chapter*{Abstract}
This project targets the growing threat of online pet scams by creating an automated system to waste scammers' time and resources, aiding in scam identification and prevention. Following extensive development and testing, a four-week experimental deployment against pet scammers unveiled and refined effective scam-baiting tactics.

\chapter*{Acknowledgements}
I would like to express my special thanks to my supervisor, Dr Matthew Edwards, for his help and guidance in this project.
\\
\\
I would also like to give my thanks to School of Engineering for inviting me into UoB Computer Science OpenAI Team, which reimburses me the cost of using the different APIs of OpenAI.
\\
\\
Sign up for GitHub Student Developer Pack provides me free access to many useful tools, involving \href{https://www.mailgun.com}{Maligun} and \href{https://www.name.com}{Name.com}. Appreciate the resources it provides!

\makedecl


\tableofcontents
\listoffigures
\listoftables

\chapter*{Ethics Statement}
An ethics application for this project was reviewed and approved by the faculty research ethics committee as application 17599.


\chapter*{Supporting Technologies}
I created my system based on an open-source GitHub repository from \url{https://github.com/an19352/scambaiter_back}. By extending and altering its code to fit my needs.
\\
\\
I integrated two GPT models, specifically GPT-3.5-turbo-0125 and GPT-4-0125-preview, into my system via their APIs to ensure the functionality of an automatic email reply feature. One necessary API key was acquired and configured using the resources available at \url{https://openai.com/}.
\\
\\
I applied my own domain name from \url{https://www.name.com/} in order to generate a large number of bait-email addresses.
\\
\\
I obtained a mailgun API key and build a mailbox server on \url{https://www.mailgun.com/}, which can support emails receiving, sending and tracking.

\chapter*{Notation and Acronyms}
\begin{quote}
\noindent
\begin{tabular}{lcl}
AFF                 &:     & Advance Fee Fraud    
 \\ 
HTTP/HTTPS          &:     & Hypertext Transfer Protocol/Hypertext Transfer Protocol Secure
 \\ 
HTML                &:     & Hypertext Markup Language
\\
XML                 &:     & Extensible Markup Language
\\
Regex               &:     & Regular Expression
\\
NLP                 &:     & Natural Language Processing
\\
SSL                 &:     & Secure Sockets Layer
\\
JSON                &:     & JavaScript Object Notation
\\
A*                  &:     & A-star
\\
GPT                 &:     & Generative Pre-Trained Transformer
\\
API/APIs            &:     & Application Programming Interface
\\
URL/url             &:     & Uniform Resource Locator
\\
UNIX/UNICS          &:     & UNiplexed Information Computing System
\\
DNS                 &:     & Domain Name System
\\
CSS                 &:     & Cascading Style Sheets
\\
DOM                 &:     & Document Object Model


\end{tabular}
\end{quote}


\mainmatter

\chapter{Introduction}
\label{chap:context}
In the definition of online scam, both advance fee and non-delivery frauds can be defined as a social engineering attack. The former is that fraudsters often promise their victims goods or more money in order to convince them to pay a fee upfront \cite{claude_toward_2014}. This scheme has been widely applied to lottery scams, inheritance scams, romance scams, etc. While the latter can be seen as a concrete realisation of the former that victims could never receive the goods even though they have sent the payment \cite{whittaker_understanding_2020}. In recent years, the happen of these two types of fraud have become increasingly frequently with the growing demand for online shopping and the development of logistics. According to the Internet Crime Report for 2019 \cite{noauthor_2019_nodate}, more than 60,000 Americans experienced non-delivery scams during the year, and another 14,607 Americans reported that they had been subjected to the AFF. The total amount of money involved in both reached nearly \$300 million. The newly released annual report for 2023 shows this amount has risen to \$450 million, even as the complaint count for the corresponding type of fraud has decreased \cite{noauthor_2023_nodate}. Simultaneously, the UK is also witnessing a similar rise in the prevalence of AFF with a substantial 549\% increase noted in March 2023 compared to three years prior \cite{stripe_crime_2023}. 
\\

Pet fraud is a typical scam involving AFF and non-delivery frauds. Scammers usually register a domain name at a very low cost to create their own e-commerce website. They attract consumers to purchase non-existent pets by placing a large number of online advertisements on social media such as Facebook and offering low prices \cite{price_resource_2020}. In order to quickly gain the trust of potential victims, scammers build seemingly legitimate websites. They prominently display on the homepage that they have joined some credible real organisations, or claim to be certified as top sellers of certain dog breeds, and display some fake consumer purchase reviews and services they have provided \cite{price_resource_2020}. In addition, most of the cute pet photos displayed on scam websites are sourced from other legitimate platforms \cite{better_business_bureau_bbb_2017}. Scammers attach the name, breed, age, and even fabricated stories about the pet alongside these stolen photos. People interested in owning pets are more likely to desire a purchase after seeing this information. They typically reach out to the scammers via the WhatsApp number, email address, or other contact details provided on the webpage. Alternatively, they may actively complete the contact form on the webpage.
\\

At this stage, most victims are almost trapped in pet scams. Scammers guide them towards private communication channels that are not supervised by regular e-commerce platforms, using pre-written scam templates to make false promises \cite{ipata_current_nodate}. Once trust is established, victims are asked to pay for pets using non-refundable payment methods like Western Union or MoneyGram, subsequently receiving a tracking number and a URL for a delivery website to track their orders \cite{price_resource_2020}. Unexpectedly, these websites are also controlled by the scammers. These 'pet orders' soon encounter various fictitious transportation and medical issues, resulting in continuous delays in the transaction \cite{price_resource_2020}. Capitalising on the victims' emotional investment, the scammers continuously request additional fees with new excuses until the victims realise they have been deceived. Consequently, the victims not only fail to receive the pets but also lose a significant amount of money. A report indicates that about two-thirds of scam victims in England and Wales have experienced mental health and sleep issues \cite{shaw_report_2024}, which is confirmed by Kanetria Hutcherson's experience \cite{better_business_bureau_bbb_2017}.
\\

The earlier data comes from a 2015 report by the Federal Trade Commission (FTC) in the United States, which recorded approximately 37,000 pet-related complaints \cite{better_business_bureau_bbb_2017}. The Better Business Bureau's (BBB) recent research indicates that, since 2017, pet scams have represented a quarter of online shopping fraud cases in the US, with the average yearly financial loss for victims continuing to rise \cite{better_business_bureau_bbb_2022}. Similar problems have been reported worldwide in recent years. For instance, Lloyds Bank in the UK noted a significant rise in pet scam complaints according to their released statistics \cite{lloyds_bank_fraudsters_2023}. Relevant research in Australia and Canada also reflects this trend \cite{better_business_bureau_bbb_2017}.
\\

Despite the above reports have revealed the mature modus operandi of pet scam gangs and their increasingly detrimental impact on society, pet scams have not received as much attention as other similar scams, such as romance scams. Current research and countermeasures are primarily focused on helping the public identify and prevent pet scams. This includes the reporting and searching functions provided by \href{https://www.ipata.org/pet-scams}{IPATA} and \href{www.petscams.com}{Petscams}, as well as \href{www.aa419.org}{AA419}, which operates a fake website database. Moreover, there are some technical development projects, such as the development of \textit{Scammed Posts Detector} \cite{norazman_development_2014} and the research of \textit{Automated classification of pet scam websites} \cite{mehmedov_automated_2021}. However, there is still a lack of targeted research that can effectively defend and disrupt such scams.
\\

This article will further investigate the field of pet scams. Based on the current operational mode of pet scams, an automatic response system that disrupts pet scams has been developed. This system is built on the basis of an expandable Scam-baiting Mail Server \cite{an19352_an19352scambaiter_back_2023}, aiming to contribute to the resistance and disruption of pet scams. As part of the project, multiple web crawlers will be built to extract and process various data. This includes the homepage URLs and the contact form pages on pet scam websites, as well as the implementation of automatic form filling. On the other hand, the system also needs the functionality to send and receive emails, as scammers will initiate email contact after receiving the form application. Thus, we conducted a 31-day experimental interaction with pet scammers by constructing our own email server and devising various response strategies, namely Investigator, Newbie, Bargainer, and Impatient Consumer. After the experiment, relevant information and data will be integrated and analysed to determine the most effective response strategy. Finally, the project results are discussed comprehensively in terms of the success rate of script execution and the response rate of emails. The project execution intends to provide the public with powerful anti-pet scam strategies directly. Additionally, the automatic form-filling script can be paired with other advertisement scam detectors, such as the \textit{Scam Detection in Online Classified Advertisements} developed by Hamad Alsaleh \cite{alsaleh_scam_2017}. This can further assist more researchers and anti-fraud organisations in understanding and combating online fraud.
\\

Therefore, the specific objectives of this project can be summarised as the following key points:

\begin{itemize}
  \item Implement the automatic form-filling script and prove its feasibility through experimentation.
  \item Identify the best personality to disrupt pet scams, contributing to a safer online environment.
  \item Increase the understanding of pet scammers and their common methods used in emails, to help myself and others improve awareness of pet scams.
\end{itemize}


\chapter{Technical Background}
\section{Petscams.com}
The data for pet scam websites in this study was sourced from a volunteer organisation called \href{https://www.petscams.com}{Petscams}. This organisation operates a website, using the same domain name, to list fraudulent pet sales and shipping sites \cite{brady_fighting_2024}. \href{https://www.petscams.com}{Petscams.com} is an effective and free platform offering a range of security services to alert the public about the danger of those websites. These services include victim reporting and comment systems, fraudulent website analysis, verification, classification, and continuous article updates. They also provide access of these data to law enforcement agencies to better disrupt and combat scammer activities. Beyond that, they have been a valuable resource for other researchers. For instance, their victim report system significantly contributed to a pet scam case study published in 2020 \cite{whittaker_understanding_2020}.
\\

Figure 1 illustrates how articles on this website maintain consistent HTML structural features in the developer mode of the Chrome browser. It's clear to see each article is nested within an $<$article$>$ tag, which is part of the HTML5 semantic markup, used to define independent content modules on the website. Importantly, each article's title, which the suspected fraudulent domain information, is placed within the $<$h2$>$ tag. 
\begin{figure}[h]
\centering
\includegraphics[width=0.8\linewidth,height=0.285\textheight]{pic/figure1.png}
\caption{Figure 1: Petscans.com in the developer mode}
\label{fig:pic1}
\end{figure}

Furthermore, as shown in Figure 2 and Figure 3, the page layout across different articles is also remarkably uniform. Each article segments the key points and introductory content, ensuring information is presented clearly. The breakdown of each scam website in the article consists of four main parts - identity, review credibility, legality, and recommended next steps - for in-depth description and analysis. This thoughtful design considerably assists victims and researchers, in quickly and fully understanding the workings of pet scams.
\begin{figure}[h]\ContinuedFloat
    \centering
    % Second subfig
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\linewidth]{pic/figure2.png}
        \caption{Figure 2: Scam - Homeafc.com}
        \label{fig:pic2}
    \end{subfigure}
    \hfill % Used to add some space between subgraphs
    % Third subfig
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\linewidth]{pic/figure3.png}
        \caption{Figure 3: Scam - Outwestschnauzers.com}
        \label{fig:pic3}
    \end{subfigure}
    \caption{Figure 2 and Figure 3}
    \label{fig:main2}
\end{figure}


It is indicated that the function of ‘robots.txt’ is to tell search engines which URLs in the site are allowed for crawler access \cite{noauthor_robotstxt_nodate}. By visiting \url{https://petscams.com/robots.txt}, you can understand the scope of web crawling permissions set by the developers, and the text data returned in the page is as follows:
\vspace{10pt}
\noindent\hrule  
\begin{Verbatim}[fontsize=\small]
# START YOAST BLOCK
# ---------------------------
User-agent: *
Disallow:
Sitemap: https://petscams.com/sitemap_index.xml
# ---------------------------
# END YOAST BLOCK
\end{Verbatim}
\hrule  
\vspace{10pt}


It can be seen that the configuration of the file is highly open. The rules specified by \href{https://www.petscams.com}{Petscams.com} permit all search engine crawlers to access all the content on the website, providing a solid foundation for the implementation of web scraping technologies.
\\

\section{Web Scraping}
Compared to traditional manual data collection, web scraping can automatically retrieve unstructured data from a large amount of text such as HTML, then convert it into a structured format and store it locally \cite{khder_web_2021}. As Zhao stated \cite{zhao_web_2017}, the web scraping process mainly includes two parts: acquiring web resources and extracting specific information from the received data. The former refers to accessing the target webpage via the HTTP protocol, while the latter is to parse and extract the necessary data through automated scripts, and then save it in a specific format for further analysis. In this section, we will focus on discussing the most commonly used technology libraries when performing these two stages.
\\

In the first stage of web scraping, communication with the web server needs to be conducted through the HTTP, which is a request-response protocol that supports most web pages \cite{chandra_python_2015}. The Requests library in Python simplifies interaction with the target URL, providing efficient and reliable methods. As one of the most popular HTTP libraries, it supports the use of various HTTP methods, including GET, POST, PUT, and DELETE. Moreover, it also includes advanced features such as handling error and exception, authentication, redirects, sessions and SSL verification \cite{noauthor_requests_nodate}. These features meet all the requirements of the initial steps, ensuring the simplification of obtaining and processing server responses, including headers, session cookies, and status codes.
\\

For the second stage, various tech libraries can be utilised to parse and extract web data. Currently, BeautifulSoup, Selenium, and Scrapy are widely used because of their powerful capabilities. These tools focus on interactions with websites, as well as the extraction and parsing of data, each one demonstrating its own performance and advantages.
\\

BeautifulSoup is a Python library designed for parsing HTML and XML documents. It offers a simple and user-friendly syntax, enabling users to swiftly locate the necessary elements in the parsing tree using selectors like tag names, id names, etc \cite{chandra_python_2015}. For small or medium-sized crawling tasks on static web pages, BeautifulSoup can often be paired with the Requests library to form a lightweight crawling solution. It's important to note that this technology library primarily focuses on parsing work and does not involve constructing concurrent requests \cite{fariha_beautifulsoup_2023}.
\\

Similarly, Selenium does not support concurrent requests, which limits its efficiency in asynchronously executing multi-site scraping tasks to a certain extent. However, as a browser automation tool, Selenium supports multiple programming languages including Python, Java, and C\#, demonstrating unique capabilities in automating web browser operations \cite{fariha_beautifulsoup_2023}. Due to the unique support for JavaScript dynamically loaded pages \cite{fariha_beautifulsoup_2023}, it specialises in simulating human user actions, such as scrolling through pages, filling out forms, and clicking buttons. Additionally, unlike tools that need to be combined with the Requests library to send HTTP requests, scripts written by Selenium can run independently, using its built-in browser control functions to directly access and operate web pages. As the automated form filling will be introduced in project development, Selenium will become an indispensable tool library.
\\

Scrapy is a web crawler framework written in Python, which includes a complete set of web scraping solutions, including request handling mechanisms, data extraction, and database storage \cite{noauthor_intro_nodate}. Utilising the Twisted asynchronous network framework, Scrapy supports efficient concurrent request handling, making it particularly suitable for large-scale data crawling tasks and complex web data collection projects \cite{noauthor_intro_nodate}. Apart from being less self-sufficient in handling JavaScript than Selenium, and new users may encounter a certain learning curve, Scrapy better integrates the key functions of both Requests and BeautifulSoup. Thus, it can complete tasks independently in most scenarios without relying on external libraries.
\\

However, the demand of this project is relatively simple, there is no need to handle a large number of concurrent requests or complex data crawling. Therefore, I finally chose Requests, BeautifulSoup and Selenium as tools for the crawler development, since our main focus is to prioritise the smooth execution of the overall system process. While this approach may not be as comprehensive as Scrapy, it offers sufficient flexibility and convenience to meet project requirements. If necessary, it has also kept a door open for incorporating more efficient and complex crawling technologies in the future.


\section{Recognition and Processing of Text Data}
Throughout the construction and execution of the project system, various types of text data will be encountered, such as form content, email content, and time records. To manage these data effectively, this section introduces various methods for implementing information retrieval, guided by the heuristic search algorithm concept. These methods can be used separately or in combination, to ensure that the system identifies and processes text data accurately and efficiently.
\\

Heuristic search is a goal-oriented searching method. While it doesn't always guarantee the optimal solution, it usually provides high-quality solutions \cite{a_chapter_2001}. The heuristic search algorithm, also known as the A* algorithm, merges the benefits of keyword search and clustering. When dealing with NP problems that have incomplete solving conditions, the following evaluation function is established \cite{zhao_information_2014}:
\\

f(n)=g(n)+h(n)
\\

Here, f(n) represents the total estimated cost of node n, g(n) represents the actual path cost from the beginning state to node n, and h(n) stands for the heuristic estimated cost from node n to the goal. These values can help reflect the distance between the node and the goal, guiding the optimal path with each search step. This algorithm improves search efficiency by avoiding the exploration of invalid paths, thus improving the accuracy of vital information retrieval while ensuring the response speed. Therefore, the essence of heuristic search is to use limited information to achieve the search goal at the lowest cost. After acquiring substantial data through web scraping, this logic can also facilitate the fast batch retrieval of relevant information. Based on the characteristics or patterns of the data items, you can predefine the keyword groups, key data types, and different pattern matches. This allows scripts to execute "semi-structured" searches, effectively recognising and filtering vast amounts of text \cite{manning_introduction_2009}.
\\

Furthermore, the use of Regex and the FuzzyWuzzy library also demonstrates the application of heuristic search. As Uzum's report \cite{uzun_comparison_2018}, Regex is a sequence of characters that define a search pattern. With the introduction of the "re" module in Python, a pattern is predefined to extract data. However, this study also found that data extracted using Regex is only 43.5\% accurate compared to the expected data. Therefore, this project will consider using only simple Regex rules to remove unnecessary text information. 
\\

FuzzyWuzzy is a Python library used for string matching. Unlike Regex, once this library is utilised, a relevance score ranging from 0-100 need to be set \cite{majumder_fuzzywuzzy_2021}. The higher the score, the more closely the query word should match the target string. This can also be viewed as the retrieval constraints defined in retrieval heuristics \cite{yang_secure_2021}. By using fuzzy string matching, this heuristic method allows for a wider range of possibilities for a target string with uncertain content, thereby preventing over-matching a single query word.


\section{GPT Models}
In response to the demand for generating a large number of emails, the conventional approach involves using predefined text templates to generate email content. However, for a more advanced automated response system, I decided to integrate the GPT model.
\\

GPT, short for Generative Pre-Trained Transformer, has been extensively pre-trained on natural language data. It can predict and generate subsequent content after receiving text, images, or sound through the transformer \cite{3blue1brown_but_2024}. Since the release of the first GPT-1 version in June 2018, OpenAI has developed numerous models with various functions \cite{noauthor_openai_nodate}. For instance, the GPT series focuses on understanding and generating natural language or code; DALL·E specialises in image generation and editing; Whisper, a voice recognition model, and Jukebox is a music generation model. And these models are still being continuously updated and optimised.
\\

The core requirement of this project is to effectively understand and generate natural language text. In this case, the newly released GPT-3.5-turbo-0125 and GPT-4-0125-preview models both meet this requirement well. The processing capabilities of these two models have reached up to 60,000 tokens (approximately 45,000 English words) and 300,000 tokens (approximately 225,000 English words) per minute, respectively. The latter can also reduce the "laziness" of the model not completing tasks \cite{noauthor_openai_nodate}. This demonstrates that by integrating the API key into the system, adjusting both the necessary prompts and other parameters according to the code template provided by OpenAI \cite{noauthor_openai_nodate}, AI can automatically generate email content in batches using human language logic. Which will result in an efficient response speed and a natural interaction experience.




\chapter{Project Execution}
\section{Automatic Receipt of Inbound Emails}
Due to the large amount of email correspondence generated when the system induces fraudsters, the demand for the domain mail server has become a primary issue. In this project, two domains called “liveonline.ninja” and “malabaka.social” were registered on \href{https://www.name.com}{Name.com}, which were used for the system's testing and experimental stages in turn. \href{https://www.mailgun.com}{Maligun.com}, as a scalable email sending platform, is a significant tool for executing this project. Under the guidance of the official documentation \cite{noauthor_domain_nodate}, two domains were added in Mailgun and configured the DNS records to track the key information of all email correspondence under the domain.
\\

As shown in Figure 4, the Logs page of Mailgun allows users to view and track all email content and their statuses under the specified domain within the past five days of communications. However, these log records are automatically deleted after five days. To ensure the 31-days experiment can proceed smoothly, a script named \texttt{email\_crawl.py} was created for automated retrieval and long-term storage of inbound emails. First, configure the Mailgun API key and the required domain name in the system. Then, use the \texttt{request.get()} method to batch transfer emails based on a customised time range and event status. Each email should be named according to its unique receiving time, represented by the Unix timestamp. To ensure readability of the archived email content, the program formats each inbound email by retaining only the title, body, sender, and recipient information. This information is finally stored in a separate JSON file.
\begin{figure}[h]
\centering
\includegraphics[width=0.85\linewidth,height=0.2\textheight]{pic/figure4.png}
\caption{Figure 4: Mailgun logs page of malabaka.social}
\end{figure}


\section{Implementation of Web Scraping}
In this project, web scraping is primarily used to access, collect, and fill in a large amount of data on the form pages of pet scam websites. We have learned before that \href{www.petscams.com}{petscams.com} is a key resource, dedicated to collecting and reporting information on scam websites related to pet sales and transportation. Given that pet transportation scams are often only involved in the later stages of scam activities, and these scam websites do not expect consumers to contact them proactively. Therefore, our crawlers will only focus on those fraudulent pet websites involved in sales.

\subsection{Crawling and Filtering List of Target Websites}
The scraping process begins with \texttt{petscam\_crawl.py}, which targets fake pet sales websites on \url{https://petscams.com/category/puppy-scammer-list/}. It fetches HTML pages using the requests library and parses them with BeautifulSoup using 'html.parser'. The script identifies each $<$article$>$ tag, then uses the $<$time$>$ and $<$h2 class="main-title"$>$ tags within to filter articles by publication date and extract scam domain names.  Each extracted domain is automatically prefixed with 'https://' to ensure they are directly accessible.
\\

While randomly visiting some of the URLs reported in recent months, it was found that some of them have already expired or been suspended. This issue is more prevalent with scam domain names revealed earlier. As a result, the code includes checks for the return status code of the link and potential message prompts such as "Account Suspended" and "Maintenance mode is on". On the other hand, the administrators of \href{www.petscams.com}{Petscams.com} occasionally re-post the same websites, which can lead to duplicate links saved in the JSON list. Taking advantage of the characteristic that sets do not allow duplicate elements \cite{sturtz_sets_nodate}, all valid URLs are first stored in a set, then converted into a list format and stored in a JSON file. These strategies can effectively prevent the potential waste of time and decrease in accuracy that invalid or duplicate links could cause in the subsequent code execution process.
\\

Next, heuristic search will be used to retrieve the contact form pages that exist in the website list collected in the previous step, which is done by a crawler program called \texttt{contactpage\_crawl.py}. Based on observations of a large number of pet scam sites, scammers usually set up a "Contact Us" page, guide people to leave their contact information, and thus get in touch with the victims separately. Through Chrome's developer mode, I summarised the three common structures they often use when developing this page, and wrote the code according to them.
\\

Some scam websites place the contact form directly on the main page instead of setting up a separate submission page. Since $<$form$>$ element is frequently used to initiate and nest form content \cite{noauthor_how_2024}, the script first checks for the existence of $<$form$>$ and common elements within it, like $<$input type = "email"$>$, on the main page. If these exist, the main page's URL is directly used as the target form page URL and stored in a JSON file. If these elements are not found on the homepage, the script continues to search for links containing specific keywords, such as "contact-us" or "reach-us". The form links related to these keywords are usually found in the $<$a href$>$ tag. Moreover, some scammers choose to put the form URL contained in the $<$a href$>$ tag in the $<$li$>$ tag instead of $<$form$>$. Thus, the code also includes keyword retrieval for this potential structure.

\subsection{Automated Form-Filling}
After identifying the URL of a scam website's contact page, the system simulates a typical scam process. This is done by a script called \texttt{formfill\_crawl.py}. It uses the Selenium library to render websites in real-time, capturing and dynamically filling form fields generated by JavaScript. Before coding, I analysed multiple pet scam websites that were recently exposed by Petscams.com to identify common patterns in form development. Typically, these forms require basic information such as name, phone number, email address, home address, pet name, and message. However, the illogical nature of these code can only be revealed in developer mode. For instance, some DOM tree structures are overly complex, causing form data to be buried within irrelevant levels or appearing in peer structures where it shouldn't be. Additionally, it is found that some data types are mismatched. For example, a phone number field that should be labelled as "tel" or "number" is incorrectly marked as a "text" type. Multiple fields are also named with unrelated long strings. All these irregular designs aim to disrupt the detection efficiency of automation tools.
\\

In response to these challenges, I abandoned the traditional data recognition method limited to standardised forms and instead implemented a complex dictionary nesting structure \cite{noauthor_python_2023}. This approach enables the specification of detailed recognition rules for each form field, including element types, required attributes, and other common optional attributes. The library of recognition rules has been progressively refined through extensive examination and analysis of fraudulent forms. Meanwhile, the script utilises the FuzzyWuzzy library for fuzzy matching, with the matching threshold set at 90, thereby improving the accuracy of recognition. And the maximum search recursion depth is set to 2, which is sufficient for handling complex multi-level form structures. To enhance efficiency, caching and multi-threading technologies have been introduced, allowing simultaneous processing of different form elements, thereby optimising the utilisation of computing resources elegantly.
\\

In the process of auto-filling forms, it is crucial to maintain the reasonableness and randomness of data to prevent potential issues, such as bothering unrelated individuals. Personal details like names are randomly created by Python library called 'names', while email addresses are a combination of random strings and the preset domain name. Even pet names and species are randomly generated. When dealing with logically connected information like phone numbers, addresses, countries, and cities, consistency at the national level is maintained. By using counters and for-loops, the system assigns specific information of either the US or the UK to different forms, thus preventing data confusion within the same form. Furthermore, the message board input uses the GPT-3.5 language model and sequentially applies four common strategies to interact with scammers. This approach ensures equal participation opportunities for responders with different personalities. For forms with drop-down selections and check-boxes, the script will automatically and randomly choose options after identifying labels such as "checkbox", "radio", or "select".
\\

Next, \texttt{formfill\_crawl.py} uses Selenium to find and click the submit button, and to check if the form has been sent successfully. The program intercepts network requests of XMLHttpRequest through JavaScript code. It captures and analyses request data in real-time, specifically focusing on the status code and response content of POST requests. This helps to determine whether a form submission has been successful or not. If the network data is not enough to confirm that, the system look for other signs like "Thank you" or "success" on the web page. In the end, all important data collected during this process, including the name, email address, scam form URLs, and the time of form filling, are recorded to keep our operation stats up to date. By extracting and testing three sets of real scam forms using the "liveonline.ninja" domain name, the script achieved a success rate between 45\% and 60\%. Although this didn't meet an expected goal, it is adequate to support the subsequent experimental research needs for this project.


\section{Implementation of Email Automation Responses}
In the email automatic response phase, the system adopted four response strategies that were initially implemented in the message board function: Investigator, Newbie, Bargainer, and Impatient Consumer. These strategies were chosen based on a study of numerous online shopper personalities \cite{noauthor_types_nodate} and a thorough analysis of past pet scam cases \cite{whittaker_understanding_2020}. Our goal is to efficiently waste the time of pet scammers. Emails require dealing with more complex text compared to message boards. For this reason, we utilised GPT-4-0125-preview, the latest advanced text generation model, to satisfy the need for high-quality natural language generation.
\\

In order to safeguard response authenticity and privacy, our system uses regular expressions to eliminate confusing greetings from the start of emails. Also, the system has adjusted the key parameters of the generation model: \texttt{temperature} and \texttt{top\_p} to control the predictability and diversity of the output \cite{noauthor_openai_nodate}. For instance, we set higher values (0.6 and 0.7) for newbies and impatient consumers, enhancing reply naturalness. For bargainers and investigators, we use medium (0.5) and lower (0.3) values respectively, aligning with their communication styles. These adjustments within the GPT language model template \cite{sanders_how_2023} enable us to customise email reply strategies. Take the following code of the function investigator as an example:
\vspace{10pt}
\noindent\hrule 
\lstset{
  basicstyle=\small\ttfamily,
  breaklines=true, 
  showstringspaces=false,
  tabsize=2,                  
  columns=fullflexible, 
}

\begin{lstlisting}[language=Python]
def investigator(prompt):
    prompt = "Reply without any signature :" + prompt
    completion = openai.ChatCompletion.create(
        model="gpt-4-0125-preview",
        messages=[
            {"role": "system",
            "content": "Act as an inquisitive investigator, seeking detailed evidence to confirm the pet sale's validity. Deny being a robot or causing trouble if questioned. Persist in the discussion, avoiding repeat questions and maintaining a logical flow with relevant follow-ups or new queries. If receiving attachments, don't open them and respond that you cannot open them. Maintain privacy and do not use any form of greeting or address such as 'Dear [name]' or 'Hi [name]'. But you should also make all your reply seem a little more humane."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.3,
        top_p=0.3
    )
    res = completion.choices[0].message.content
    res = re.sub(r"\[.*?\]", "", res)
    return res.strip()
\end{lstlisting}
\hrule  
\vspace{10pt}

The \texttt{corn.py} script is primarily used for centralised replies to batch emails. It parses emails collected by \texttt{email\_crawl.py}, generates appropriate reply text, and then uses the Mailgun module to send the email. To maintain a consistent and credible conversation, this script identifies the proper response strategy by searching the record from the previous script execution. This approach can avoid suspicion caused by sudden strategy changes. Each generated email uses formatting techniques, by simulating the ongoing conversation with "Re:" as the prefix, then adding the signature and username at the end to enhance the authenticity of the response.
\begin{figure}[h]
\centering
\includegraphics[width=\linewidth, height=0.25\textheight]{pic/figure5.png}
\caption{Figure 5: A scam email conversation for testing}
\label{fig:pic5}
\end{figure}

As shown in Figure 5, the system acts as a "bargainer" in response to a test scam email delivered to the system domain via Gmail. Figure 6 provides detailed metadata of this email, validating the system's proficiency in handling nuanced communication tasks. It has revealed that the content matching and email format meet the test benchmark, effectively demonstrating the practicality of the script in real email exchanges. In order to prevent system overload and potential crashes, the script also sets a maximum limit on the number of emails and content tokens, thereby effectively managing the load. Simultaneously, it employs an exception-catching mechanism to avoid issues caused by errors.
\begin{figure}[H]
\centering
\includegraphics[width=0.4\linewidth]{pic/figure6.png}
\caption{Figure 6: Metadata of this response email}
\label{fig:pic6}
\end{figure}

\section{Construction and Operation of the System}
The system was built on an open-source, expandable scam-baiting mail server \cite{an19352_an19352scambaiter_back_2023}, which eliminates the need to construct a basic architecture from scratch. The simplified development process involved integrating own crawlers, customising the reply procedure for pet scammers and configuring API keys along with data storage directories. Consequently, this section mainly focuses on how these pre-written programs are well organised into a complete system designed to disrupt pet scams.
\\

Despite comprehensive testing of all programs before the experiment's start, real-time error monitoring was vital to ensure an uninterrupted experiment. Instead of merging the five key scripts or enhancing their calling, they were managed and run separately within the project. However, there is an inherent link between these scripts, each producing data stored in JSON format in the system, ready for others to access. After eliminating two irrelevant crawlers and several email reply templates from the original email server, Figure 7 and Figure 8 clearly show the architecture and operation flow of the system during the two stages of web interaction and email interaction respectively.
\begin{figure}[H]
\centering
\includegraphics[width=0.8\linewidth]{pic/figure7.png}
\caption{Figure 7: Stage 1 - Web Interaction}
\label{fig:pic7}
\end{figure}

\begin{figure}[H]\ContinuedFloat
\centering
\includegraphics[width=0.8\linewidth]{pic/figure8.png}
\caption{Figure 8: Stage 2 - Email Interaction}
\label{fig:pic8}
\end{figure}

Figure 7 illustrates the system's first stage workflow, specifically, the interaction process with pet scam pages. This flowchart clearly outlines each step, from initial crawling to data processing. Each step involves storing the URL crawling results, regardless of success. All data is saved in JSON format. The system also calculates the success rate of each script based on the number of successfully captured links and attempts, which is vital for monitoring and optimising the crawler's performance. The entire process's progression relies on accurately reading and processing valid data.
\\

Figure 8 shows the second stage of the email handling process, which involves processing emails from the Mailgun domain server. During this stage, all incoming scam emails are archived for future review and sorted into the processing queue. In addition, each email's recipient information is compared with the data in the \texttt{cache.json} file from the first stage. If the data matches, it means that the email originated from the form-filling activity in the first stage. Details from matched emails, such as the email address of scammers and the timestamp of the email, are automatically stored in the \texttt{record.json} file. This file also contains the reply strategy for deciding whether to reply to the email, which is executed through the corn.py script. After each reply to a new scammer, the \texttt{history.json} file is updated to track the usage frequency of various reply strategies.
\\

After finishing the coding and thorough testing of the project, I scheduled the experiment to start on March 15th (Friday) and end on April 15th. Regular visits to \href{www.petscams.com}{petscams.com} showed that the website updates approximately every 2-3 days, with each update publishing around 15 articles exposing fraudulent pet sales platforms. Given this pattern, and considering the success rate of \texttt{formfill\_crawler.py}, I divided the 31-day experiment into four cycles. Apart from the first Friday of the first week, when it was necessary to interact with pet scam pages exposed within the past 30 days, the following three Fridays required interactions with pages exposed within the past 7 days only. This approach ensured continuous communication with the scammers throughout the experiment and facilitated the collection of diverse data. Also, during the experiment, I needed to respond to new emails daily between 5pm and 7pm. Accounting for the 4-5 hour time difference, this time corresponds to 12pm to 3pm Eastern Time in the United States, a period of high activity for Americans. In the previous text, we have already learned that the United States is not only one of the first countries to expose online pet scams but also the country where these scams occur most frequently. Therefore, sending emails during this time period increases the likelihood of timely responses, even from scammers located in Europe. More detailed information about the experimental process is shown in Figure 9.
\begin{figure}[H]\ContinuedFloat
\centering
\includegraphics[width=\linewidth]{pic/figure9.png}
\caption{Figure 9: The Process of Experiment}
\label{fig:pic9}
\end{figure}


\chapter{Analysis of results}



\chapter{Critical Evaluation}









\chapter{Conclusion}
In conclusion, this project effectively combats the rise of online pet scams by employing an automated system designed to deplete scammers' resources and deter fraudulent activities. Over a four-week experimental phase, four robotic responders consistently engaged scammers, wasting at least a month of their time. Dozens of scammers continued to interact with the system even after the experiment concluded. This not only aids in scam prevention but also refines scam-baiting strategies, contributing valuable insights into the fight against non-delivery fraud and advance fee fraud.
\\

The future direction of this project may focus on integrating the script for filling non-delivery fraud websites with other systems that can classify fraudulent advertisements or websites. This could include the code for automatic detection of pet scam websites \cite{mehmedov_automated_2021}.



% \bibliographystyle{ieee}
\bibliography{references}

\appendix

\chapter{Appendix A: }
\label{}






\end{document}
